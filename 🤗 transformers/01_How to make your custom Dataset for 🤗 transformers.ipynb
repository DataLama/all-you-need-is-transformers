{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to make your custom Data Pipeline for ðŸ¤— transformers\n",
    "transformers has their own style of Data Pipeline. When you want to implement their model to solve your own NLP problem, you should build your own custom data pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction of ðŸ¤—'s Data pipeline\n",
    "**Main Consideration of ðŸ¤—'s Data Pipeline**\n",
    "- Uniform Interface for Multiple training set. (e.g. Glue task)\n",
    "- Uniform Interface for Multiple Pretrained Language Model. (e.g. RoBerta vs Bert)\n",
    "- Make sure only the first process in distributed training processes the dataset, and the others will use the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. ðŸ¤—'s core data structure.\n",
    "- ðŸ¤— introduce their own data structure which is `InputExample` and `InputFeatures`.\n",
    "- It is defined by dataclass.\n",
    "\n",
    "#### 1.1) `InputExample`\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class InputExample:\n",
    "    \n",
    "    guid: str\n",
    "    text_a: str\n",
    "    text_b: Optional[str] = None\n",
    "    label: Optional[str] = None\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self), indent=2) + \"\\n\"\n",
    "```\n",
    "\n",
    "- The base unit to store **1 raw data**\n",
    "    - **guid** : Id of document\n",
    "    - **text_a** : string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "    - **text_b** :(Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "    - **label** : (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "            \n",
    "\n",
    "- It can be Seriealized to a Json String. (for caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This foundation is HORRIBLE i dont know why i keep seeing positive reviews.\n",
      "NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "#### The usage of InputExample\n",
    "from transformers.data.processors.utils import InputExample\n",
    "\n",
    "# Sentiment Analysis\n",
    "guid = 42\n",
    "text = \"This foundation is HORRIBLE i dont know why i keep seeing positive reviews.\"\n",
    "labels = \"NEGATIVE\"\n",
    "\n",
    "# define InputExample object\n",
    "example = InputExample(guid=guid, text_a=text, label=labels)\n",
    "print(example.text_a)\n",
    "print(example.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2) `InputFeatures`\n",
    "\n",
    "```python\n",
    "@dataclass(frozen=True)\n",
    "class InputFeatures:\n",
    "\n",
    "    input_ids: List[int]\n",
    "    attention_mask: Optional[List[int]] = None\n",
    "    token_type_ids: Optional[List[int]] = None\n",
    "    label: Optional[Union[int, float]] = None\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self)) + \"\\n\"\n",
    "```\n",
    "\n",
    "- The base unit to store **1 featured data** (text is tokenized and transformed for Inputs)\n",
    "    - **input_ids** : Indices of input sequence tokens in the vocabulary.\n",
    "    - **attention_mask** : Mask to avoid performing attention on padding token indices. Usually  ``1`` for tokens that are NOT MASKED, ``0`` for MASKED (padded) tokens.\n",
    "    - **token_type_ids** : (Optional) Segment token indices to indicate first and second\n",
    "            portions of the inputs. Only some models use them.\n",
    "    - **label** : (Optional) Label corresponding to the input. Int for classification problems, float for regression problems.\n",
    "    \n",
    "    \n",
    "\n",
    "- It can be Seriealized to a Json String. (for caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids -> [101, 1188, 4686, 1110, 145, 9565, 20595, 13360, 2036, 178, 1274, 1204, 1221, 1725, 178, 1712, 3195, 3112, 3761, 119, 102]\n",
      "attention_mask -> [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "token_type_ids -> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label -> NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "#### The usage of InputFeatures\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "\n",
    "# load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# encode text\n",
    "encoded_input = tokenizer.encode_plus(example.text_a)\n",
    "\n",
    "# define InputFeatures object\n",
    "feature = InputFeatures(\n",
    "    input_ids = encoded_input['input_ids'],\n",
    "    attention_mask = encoded_input['attention_mask'],\n",
    "    token_type_ids = encoded_input['token_type_ids'],\n",
    "    label = labels\n",
    ")\n",
    "\n",
    "print(f\"input_ids -> {feature.input_ids}\")\n",
    "print(f\"attention_mask -> {feature.attention_mask}\")\n",
    "print(f\"token_type_ids -> {feature.token_type_ids}\")\n",
    "print(f\"label -> {feature.label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Code your own pipeline with `Processor` and `convert_examples_to_features`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1) `Processor`\n",
    "\n",
    "ðŸ¤— offers `DataProcessor`.\n",
    "\n",
    "**The Features of Processor**\n",
    "- Read Data from the file by train, dev, test and return `InputExample`.\n",
    "- You can get the label_list by `self.get_labels()`.\n",
    "- Subclass DataProcessor and overwrite your own method or property.\n",
    "- You can use Processor for TF similar manner.\n",
    "\n",
    "```python\n",
    "class DataProcessor:\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"Gets an example from a dict with tensorflow tensors.\n",
    "        Args:\n",
    "            tensor_dict: Keys and values should match the corresponding Glue\n",
    "                tensorflow_dataset examples.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of :class:`InputExample` for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of :class:`InputExample` for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of :class:`InputExample` for the test set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def tfds_map(self, example):\n",
    "        \"\"\"Some tensorflow_datasets datasets are not formatted the same way the GLUE datasets are.\n",
    "        This method converts examples to the correct format.\"\"\"\n",
    "        if len(self.get_labels()) > 1:\n",
    "            example.label = self.get_labels()[int(example.label)]\n",
    "        return example\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex) Processor for Text classification**\n",
    "- Sampled AmazonReview Dataset.\n",
    "    - Sampled 1% from `torchtext.datasets.AmazonReviewPolarity` dataset.\n",
    "- `get_*_examples()` methods are for hold-out validation.\n",
    "    - Prepare your dataset splitted by train, dev, test set.\n",
    "    - For fine-tuning, tsv format is recommended.\n",
    "- With `get_labels`, you can get the label_list.\n",
    "- Possible customization.\n",
    "    - text preprocessing\n",
    "    - cross-validation\n",
    "    - and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "from transformers import InputExample, DataProcessor\n",
    "\n",
    "class AmazonReviewSentimentAnalysisProcessor(DataProcessor):\n",
    "    def __init__(self):\n",
    "        emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "        self.eng = re.compile(f'[^ .,?!/@$%~ï¼…Â·âˆ¼()\\x00-\\x7Fa-zA-Z{emojis}]+')\n",
    "        self.url = re.compile(\n",
    "    r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "    \n",
    "    \"\"\"Processor for the Amazon Review Sentiment Analysis Data.\"\"\"\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"[Overwrite] Gets a collection of :class:`InputExample` for the train set.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"[Overwrite] Gets a collection of :class:`InputExample` for the dev set.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"[Overwrite] Gets a collection of :class:`InputExample` for the test set.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "        \n",
    "    def get_labels(self):\n",
    "        \"\"\"[Overwrite] Gets the list of labels for this data set.\"\"\"\n",
    "        return [\"NEGATIVE\", \"POSITIVE\"]\n",
    "    \n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"[Custom] Read dataset and return InputExample\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = self._preprocess(line[1])\n",
    "            label = line[0]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples\n",
    "    \n",
    "    def _preprocess(self, string):\n",
    "        \"\"\"[Custom] Preprocessing raw data with regular expression\"\"\"\n",
    "        if type(string)==str:\n",
    "            string = self.url.sub(\" \", string)\n",
    "            string = self.eng.sub(\" \", string)\n",
    "            return string\n",
    "        else:\n",
    "            return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NEGATIVE', 'POSITIVE']\n"
     ]
    }
   ],
   "source": [
    "# define processor\n",
    "data_dir = 'sample' # this could be assigned by argparse.\n",
    "processor = AmazonReviewSentimentAnalysisProcessor()\n",
    "\n",
    "# labels\n",
    "label_list = processor.get_labels()\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train set 28800\n",
      "# of dev set 3600\n",
      "# of test set 3600\n"
     ]
    }
   ],
   "source": [
    "# examples\n",
    "train_examples = processor.get_train_examples(data_dir)\n",
    "print(f\"# of train set {len(train_examples)}\")\n",
    "dev_examples = processor.get_dev_examples(data_dir)\n",
    "print(f\"# of dev set {len(dev_examples)}\")\n",
    "test_examples = processor.get_test_examples(data_dir)\n",
    "print(f\"# of test set {len(test_examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-42\n",
      "\"A friend gave me a copy of \"\"Dresden\"\" and I watched it many times before giving it back and ordering my own copy. I watched the movie several times for different reasons: 1. History, 2. View from the German side, 3. View from the British side, 4. Acting skills. I love the movie, am impressed with F. Woll's in the main role. I think some scenes were too Hollywood, loved the honesty of the script, not too much violence, but one can start to feel the horror of the bombings and being trapped in cellars and on the ground by the devastating fires. I was a mistake, a big one, History was destroyed as it often is in wars. Did the Germans deserve this so much emotionally human reaction? Yes and No. Not only the Germans or the people of Dresden lost a jewel.\"\n",
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "example = train_examples[42]\n",
    "print(example.guid)\n",
    "print(example.text_a)\n",
    "print(example.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2) `convert_examples_to_features`\n",
    "- Tokenize and indices InputExample to InputFeatures for input of PLM.\n",
    "\n",
    "```python\n",
    "def convert_examples_to_features(\n",
    "        examples: List[InputExample],\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        max_length: Optional[int] = None, # Maximum Sequence Length\n",
    "        pad_on_left = False, # If set to ``True``, the examples will be padded on the left rather than on the right (default)\n",
    "        pad_token = 0,\n",
    "        mask_padding_with_zero = True, \n",
    "        return_tensors = None # if 'pt' torch.Tensor elif 'tf' else List[InputFeatures]\n",
    "    ):\n",
    "    \n",
    "    ## tokenize and indices for inputs (consider your PLM inputs)\n",
    "    \n",
    "    ## transform labels for your modeling task (classification or regression or ??)\n",
    "\n",
    "    return features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [You can add convert_examples_to_features in the Processor class.](https://github.com/huggingface/transformers/blob/05810cd80a5ca83065e0dbe5335c030c4a435ddb/src/transformers/data/processors/utils.py#L124)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex) convert_examples_to_features for Text classification**\n",
    "- I highly recommend padding with the `data_collator` for saving your GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union\n",
    "from transformers import AutoTokenizer, InputFeatures, PreTrainedTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "def convert_examples_to_features(\n",
    "    examples: List[InputExample],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    max_length: Optional[int] = None,\n",
    "):\n",
    "    # Set max_len for tokenization\n",
    "    if max_length is None:\n",
    "        max_length = tokenizer.max_len\n",
    "    \n",
    "    # Define processor and labels\n",
    "    processor = AmazonReviewSentimentAnalysisProcessor()\n",
    "    \n",
    "    label_list = processor.get_labels()\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "    \n",
    "    # transform features\n",
    "    features = []\n",
    "    for ex_index, example in enumerate(examples):\n",
    "        \n",
    "        tokenized_examples = tokenizer.encode_plus(example.text_a, max_length=max_length, truncation=True)\n",
    "        \n",
    "        input_ids = tokenized_examples['input_ids']\n",
    "        token_type_ids = tokenized_examples['token_type_ids']\n",
    "        attention_mask = tokenized_examples['attention_mask']\n",
    "        \n",
    "        label_map[example.label]\n",
    "        \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids = input_ids, \n",
    "                attention_mask = attention_mask,\n",
    "                token_type_ids = token_type_ids,\n",
    "                label = label_map[example.label]\n",
    "            )\n",
    "        )\n",
    "        if ex_index < 5:\n",
    "            pass\n",
    "            # define your own logger\n",
    "#             logger.info(\"*** Example ***\")\n",
    "#             logger.info(\"guid: %s\" % (example.guid))\n",
    "#             logger.info(\"features: %s\" % features[i])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InputFeatures(input_ids=[101, 1188, 3317, 1110, 1141, 1104, 1103, 1436, 2865, 16601, 146, 1138, 1518, 1215, 119, 1135, 19819, 1139, 2555, 1121, 5917, 1272, 1122, 1144, 170, 3528, 6440, 1104, 26701, 1134, 1209, 1145, 18055, 1155, 7920, 1191, 1128, 1132, 170, 179, 8032, 2895, 119, 1109, 1902, 1110, 1145, 1304, 3505, 132, 1122, 1144, 170, 1822, 10845, 1204, 1134, 2144, 112, 189, 2845, 1229, 1128, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The example of test features\n",
    "test_features = convert_examples_to_features(test_examples, tokenizer=tokenizer, max_length=64)\n",
    "test_features[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Define Dataset\n",
    "\n",
    "It has been long journey for defining ðŸ¤— data pipelines. \n",
    "\n",
    "Let's combine all things to define Dataset. (`torch.utils.data.dataset.Dataset`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLueDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        args: BlueDataTrainingArguments,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        limit_length: Optional[int] = None,\n",
    "        evaluate=False,\n",
    "    ):\n",
    "        self.args = args\n",
    "        processor = blue_processors[args.task_name]()\n",
    "        self.output_mode = blue_output_modes[args.task_name]\n",
    "        \n",
    "        # Load data features from cache or dataset file\n",
    "        cached_features_file = os.path.join(\n",
    "            args.data_dir,\n",
    "            \"cached_{}_{}_{}_{}\".format(\n",
    "                \"dev\" if evaluate else \"train\", tokenizer.__class__.__name__, str(args.max_seq_length), args.task_name,\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "\n",
    "            if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "                start = time.time()\n",
    "                self.features = torch.load(cached_features_file)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {args.data_dir}\")\n",
    "                label_list = processor.get_labels()\n",
    "                examples = (processor.get_dev_examples(args.data_dir) \n",
    "                            if evaluate \n",
    "                            else processor.get_train_examples(args.data_dir))\n",
    "                if limit_length is not None:\n",
    "                    examples = examples[:limit_length]\n",
    "                self.features = _blue_convert_examples_to_features(\n",
    "                    examples,\n",
    "                    tokenizer,\n",
    "                    max_length=args.max_seq_length,\n",
    "                    label_list=label_list,\n",
    "                    output_mode=self.output_mode,\n",
    "                )\n",
    "                start = time.time()\n",
    "                torch.save(self.features, cached_features_file)\n",
    "                # ^ This seems to take a lot of time so I want to investigate why and how we can improve.\n",
    "                logger.info(\n",
    "                    f\"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i) -> InputFeatures:\n",
    "        return self.features[i]\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self.label_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
